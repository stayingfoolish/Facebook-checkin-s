{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prep_data(df_train,df_test,nx_cell,ny_cell):\n",
    "    print('Feature Engineering ..')\n",
    "    eps = 0.00001\n",
    "    #feature x/y\n",
    "    df_train['x_d_y']=df_train.x.values/(df_train.y.values +eps)\n",
    "    df_test['x_d_y']=df_test.x.values/(df_test.y.values +eps)\n",
    "    #feature x*y\n",
    "    df_train['x_m_y']=df_train.x.values*df_train.y.values\n",
    "    df_test['x_m_y']=df_test.x.values*df_test.y.values\n",
    "\n",
    "    #feature date and times\n",
    "    startdate=np.datetime64('2014-01-01T01:01',dtype='datetime64[m]')\n",
    "    #minutes\n",
    "    #train\n",
    "    d_times=pd.DatetimeIndex(startdate +np.timedelta64(int(mn),'m')for mn in df_train.time.values)\n",
    "    df_train['hour']=d_times.hour\n",
    "    df_train['weekday'] = d_times.weekday\n",
    "    df_train['day'] = d_times.day\n",
    "    df_train['month'] = d_times.month\n",
    "    df_train['year'] = d_times.year\n",
    "    df_train = df_train.drop(['time'], axis=1)\n",
    "    #test\n",
    "    d_times=pd.DatetimeIndex(startdate +np.timedelta64(int(mn),'m')for mn in df_test.time.values)\n",
    "    df_test['hour']=d_times.hour\n",
    "    df_test['weekday'] = d_times.weekday\n",
    "    df_test['day'] = d_times.day\n",
    "    df_test['month'] = d_times.month\n",
    "    df_test['year'] = d_times.year\n",
    "    df_test = df_test.drop(['time'], axis=1)\n",
    "    \n",
    "    #grid \n",
    "    \n",
    "    size_x=10./nx_cell\n",
    "    size_y=10./ny_cell\n",
    "    #train\n",
    "    x_x=np.where(df_train.x.values<eps,0,df_train.x.values-eps)\n",
    "    y_y=np.where(df_train.y.values<eps,0,df_train.y.values-eps)\n",
    "    positionx=(x_x/size_x).astype(np.int)\n",
    "    positiony=(y_y/size_y).astype(np.int)\n",
    "    df_train['grid_cell']=positiony*nx_cell+positionx\n",
    "    #test\n",
    "    x_x=np.where(df_test.x.values<eps,0,df_test.x.values-eps)\n",
    "    y_y=np.where(df_test.y.values<eps,0,df_test.y.values-eps)\n",
    "    positionx=(x_x/size_x).astype(np.int)\n",
    "    positiony=(y_y/size_y).astype(np.int)\n",
    "    df_test['grid_cell']=positiony*nx_cell+positionx\n",
    "    #feature normalization\n",
    "    \n",
    "    columns = ['x', 'y', 'x_d_y', 'x_m_y', 'hour','weekday', 'day', 'month', 'year']    \n",
    "    for i in columns:\n",
    "        mean=df_train[i].mean()\n",
    "        std_dev=df_train[i].std()\n",
    "        df_train[i]=(df_train[i].values - mean)/std_dev\n",
    "        df_test[i]=(df_test[i].values - mean)/std_dev\n",
    "    return df_train,df_test\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_cell(df_train,df_test,grid_id,threshold):\n",
    "    #to pick instances with more than threshold place_ids\n",
    "    #on train\n",
    "    df_cell_train=df_train.loc[df_train.grid_cell==grid_id]\n",
    "    place_id_counter=df_cell_train.place_id.value_counts()\n",
    "    selector=(place_id_counter[df_cell_train.place_id.values]>=threshold)\n",
    "    df_cell_train=df_cell_train.loc[selector.values]\n",
    "    #test\n",
    "    df_cell_test = df_test.loc[df_test.grid_cell == grid_id]\n",
    "    row_ids = df_cell_test.index\n",
    "\n",
    "    #data prep\n",
    "    le= LabelEncoder()\n",
    "    y=le.fit_transform(df_cell_train.place_id.values)\n",
    "    X=df_cell_train.drop(['place_id','grid_cell'],axis=1).values.astype(int)\n",
    "    X_test = df_cell_test.drop(['grid_cell'], axis = 1).values.astype(int)\n",
    "\n",
    "    #classifier\n",
    "    clf=xgb.XGBClassifier()\n",
    "    clf.fit(X,y)\n",
    "    y_pred= clf.predict_proba(X_test)\n",
    "    \n",
    "    pred_labels=le.inverse_transform(np.argsort(y_pred,axis=1)[:,::-1][:,:3])\n",
    "    return pred_labels,row_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_grid(df_train,df_test,df_sub,threshold,n_cells):\n",
    "    for g_id in range(n_cells):\n",
    "        if g_id % 10 == 0:\n",
    "            print('iteration: %s' %(g_id))\n",
    "    print('processing grid..')\n",
    "    pred_labels,row_ids=process_cell(df_train,df_test,g_id,threshold)\n",
    "    str_labels=np.apply_along_axis(lambda x: ''.join(x.astype(str)),1,pred_labels)\n",
    "    df_sub.loc[row_ids]=str_labels.reshape(-1,1)\n",
    "    return df_sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Feature Engineering ..\n",
      "iteration: 0\n",
      "iteration: 10\n",
      "iteration: 20\n",
      "iteration: 30\n",
      "iteration: 40\n",
      "iteration: 50\n",
      "iteration: 60\n",
      "iteration: 70\n",
      "iteration: 80\n",
      "iteration: 90\n",
      "iteration: 100\n",
      "iteration: 110\n",
      "iteration: 120\n",
      "iteration: 130\n",
      "iteration: 140\n",
      "iteration: 150\n",
      "iteration: 160\n",
      "iteration: 170\n",
      "iteration: 180\n",
      "iteration: 190\n",
      "iteration: 200\n",
      "iteration: 210\n",
      "iteration: 220\n",
      "iteration: 230\n",
      "iteration: 240\n",
      "iteration: 250\n",
      "iteration: 260\n",
      "iteration: 270\n",
      "iteration: 280\n",
      "iteration: 290\n",
      "iteration: 300\n",
      "iteration: 310\n",
      "iteration: 320\n",
      "iteration: 330\n",
      "iteration: 340\n",
      "iteration: 350\n",
      "iteration: 360\n",
      "iteration: 370\n",
      "iteration: 380\n",
      "iteration: 390\n",
      "iteration: 400\n",
      "iteration: 410\n",
      "iteration: 420\n",
      "iteration: 430\n",
      "iteration: 440\n",
      "iteration: 450\n",
      "iteration: 460\n",
      "iteration: 470\n",
      "iteration: 480\n",
      "iteration: 490\n",
      "iteration: 500\n",
      "iteration: 510\n",
      "iteration: 520\n",
      "iteration: 530\n",
      "iteration: 540\n",
      "iteration: 550\n",
      "iteration: 560\n",
      "iteration: 570\n",
      "iteration: 580\n",
      "iteration: 590\n",
      "iteration: 600\n",
      "iteration: 610\n",
      "iteration: 620\n",
      "iteration: 630\n",
      "iteration: 640\n",
      "iteration: 650\n",
      "iteration: 660\n",
      "iteration: 670\n",
      "iteration: 680\n",
      "iteration: 690\n",
      "iteration: 700\n",
      "iteration: 710\n",
      "iteration: 720\n",
      "iteration: 730\n",
      "iteration: 740\n",
      "iteration: 750\n",
      "iteration: 760\n",
      "iteration: 770\n",
      "iteration: 780\n",
      "iteration: 790\n",
      "processing grid..\n",
      "Generating submission file ...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('Loading data ...')\n",
    "    df_train = pd.read_csv('/home/prajwal/Documents/facebook /train.csv',\n",
    "                           usecols=['row_id','x','y','time','place_id'], \n",
    "                           index_col = 0,)\n",
    "    \n",
    "    df_test = pd.read_csv('/home/prajwal/Documents/facebook /test.csv',\n",
    "                          usecols=['row_id','x','y','time'],\n",
    "                          index_col = 0)\n",
    "    #Defining the size of the grid\n",
    "    nx_cell = 20\n",
    "    ny_cell = 40 \n",
    "  \n",
    "    df_train, df_test = prep_data(df_train, df_test, nx_cell, ny_cell)\n",
    "  \n",
    "    #Solving classification problems inside each grid cell\n",
    "    threshold = 500 #Threshold on place_id inside each cell. Only place_ids with at \n",
    "            #least th occurrences inside each grid_cell are considered. This\n",
    "            #is to avoid classes with very few samples and speed-up the \n",
    "            #computation.\n",
    "    df_sub = pd.read_csv('/home/prajwal/Documents/facebook /sample_submission.csv', index_col = 0)   \n",
    "   \n",
    "    df_submission  = process_grid(df_train, df_test, df_sub, threshold, \n",
    "                                  nx_cell * ny_cell)                                 \n",
    "    print('Generating submission file ...')\n",
    "    df_submission.to_csv(\"sub.csv\", index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
